---
title: "The interactive origin of iconiciy: Mixed effects models"
output: 
  pdf_document:
    toc: true
---

\newpage

# Introduction

This file contains an analysis of the spikiness ratings of the final output languages and the accuracy of guessing during the experiments.  The spikiness ratings are not bimodally disributed, so the analysis of spikiness ratings is done using both the continuous spikiness rating values and a binarised version of the ratings.

Note that in the main text, we refer to the two conditions as "communication" and "reproduction", while the data is coded as "communication" and "learning".

# Spikiness ratings

## Load libraries

```{r message=FALSE, warning=FALSE}

library(gplots)
library(lattice)
library(ggplot2)
library(lme4)
library(party)
library(sjPlot)
library(lawstat)
```

```{r echo=F}
setwd("~/Documents/MPI/MonicaIconicity/SelectionAnalysis/analysis/")

getMEText = function(r,ef, wald=NULL){

AIC = r[2,]$AIC
loglikDiff = signif(diff(r$logLik),2)
chi = round(r$Chisq[2],2)
df = r$`Chi Df`[2]
p = signif(r$`Pr(>Chisq)`[2],2)

wald.text = ""

if(!is.null(wald)){
  est = signif(wald[1],2)
  stder = signif(wald[2],2)
  t = signif(wald[3],2)
  wptext = ""
  if(!is.na(wald[4])){
    wptext = paste(", Wald p =",signif(wald[4],2))
  }
  wald.text = paste("beta = ",est,", std.err = ",stder, ", Wald t = ",t,wptext,';')
}

begin = 'There was no significant'
if(p <0.1){
  begin = "There was a marginal"
}
if(p < 0.05){
  begin = 'There was a significant'  
}


return(paste(begin,ef,"(",wald.text,"log likelihood difference =",
             loglikDiff,", df = ",df,", Chi Squared =", chi,", p = ",p,")."))
}

```

## Load data

```{r}
finalLangs = read.csv("../data/finalLanguages/FinalLanguages.csv", stringsAsFactors = F)
# convert labels to English
finalLangs$Shape[finalLangs$Shape=="Picudo"] = "Spiky"
finalLangs$Shape[finalLangs$Shape=="Redondo"] = "Round"

# load all trial data
alldatx = read.csv("../results/AllTrialData.csv",stringsAsFactors = F)
```

Center spikiness ratings and re-level factors.

```{r}
finalLangs$RatedSpikiness.center = 
  finalLangs$RatedSpikiness- mean(finalLangs$RatedSpikiness)

finalLangs$Cond = factor(finalLangs$Cond, levels=c("Learn","Communication"))
finalLangs$Shape = factor(finalLangs$Shape, levels=c("Round","Spiky"))

```

Plot the data by item (all conditions, all generations)

```{r}
par(mar=c(8,4,2,2))
plotmeans(finalLangs$RatedSpikiness.center~finalLangs$Item, las=2, xlab="", connect=F)
```

There are differences between items

## Mixed effects model

Build a series of models with random effects for Chain and Item.  


```{r}
# null model
m0 = lmer(RatedSpikiness.center ~ 1 + (1 |Chain)  + (1|Item), data=finalLangs)
# + condition
m1 = lmer(RatedSpikiness.center ~ Cond + (1 |Chain)  + (1|Item), data=finalLangs)
# + generation
m2 = lmer(RatedSpikiness.center ~ Cond + Gen + (1 |Chain) + (1|Item), data=finalLangs)
# + shape
m3 = lmer(RatedSpikiness.center ~ Cond + Gen + Shape + (1 |Chain)  
          + (1|Item), data=finalLangs)
# + interaction between shape and generation
m4 = lmer(RatedSpikiness.center ~ Cond + (Gen * Shape) + (1 |Chain)  
          + (1|Item), data=finalLangs)
# + interaction between condition and generation
m5 = lmer(RatedSpikiness.center ~ (Cond*Gen) + (Gen * Shape) + (1 |Chain)  
          + (1|Item), data=finalLangs)
# + interaction between shape and condition
m6 = lmer(RatedSpikiness.center ~ (Cond*Gen) + (Gen * Shape) + (Shape:Cond) 
          + (1 |Chain)  + (1|Item), data=finalLangs)
# + 3-way interaction
m7 = lmer(RatedSpikiness.center ~ Cond * Gen * Shape + (1 |Chain)  
          + (1|Item), data=finalLangs)
```

### Results

Look inside main model

```{r}
summary(m7)
```


Test the differences between model fits.

```{r}
anova(m0,m1,m2,m3,m4,m5,m6,m7)
```


`r getMEText(anova(m2,m3), "main effect of shape",summary(m7)$coef['ShapeSpiky',])`

`r getMEText(anova(m5,m6), "interaction between shape and condition", summary(m7)$coef['CondCommunication:ShapeSpiky',])`

`r getMEText(anova(m3,m4), "interaction between shape and generation", summary(m7)$coef['Gen:ShapeSpiky',])`

`r getMEText(anova(m6,m7), "three-way interaction between shape, condition and generation", summary(m7)$coef['CondCommunication:Gen:ShapeSpiky',])`


Plot the random effects.

```{r}
dotplot(ranef(m7, condVar=T))
```

Plot the fixed effects with error estiamtes from the final model.  The 3-way interaction between condition, generation and shape is marginaly significant:

```{r message=F, warning=F}
sjp.lmer(m7, type='fe', geom.colors=c(1,1))
```

\newpage

## Mixed effects model with binarised spikiness ratings

The spikiness ratings are not normally distributed:

```{r}
hist(finalLangs$RatedSpikiness)
```

So we binarise the variable into spiky/not spiky:

```{r}
finalLangs$RatedSpikiness.bin = finalLangs$RatedSpikiness >4
```

Run a series of models.  Note that intermediate models 5 and 6 do not converge, but the final model 7 does.

```{r}

mcontrol = glmerControl(optCtrl = list(maxfun = 500000))

mb0 = glmer(RatedSpikiness.bin ~ 1 + (1 |Chain)  + (1|Item), 
            data=finalLangs, family=binomial, control = mcontrol)
mb1 = glmer(RatedSpikiness.bin ~ Cond + (1 |Chain)  + (1|Item), 
            data=finalLangs, family=binomial, control = mcontrol)
mb2 = glmer(RatedSpikiness.bin ~ Cond + Gen + (1 |Chain) + (1|Item), 
            data=finalLangs, family=binomial, control = mcontrol)
mb3 = glmer(RatedSpikiness.bin ~ Cond + Gen + Shape + (1 |Chain)  + (1|Item), 
            data=finalLangs, family=binomial, control = mcontrol)
mb4 = glmer(RatedSpikiness.bin ~ Cond + (Gen * Shape) + (1 |Chain)  + (1|Item), 
            data=finalLangs, family=binomial, control = mcontrol)
mb5 = glmer(RatedSpikiness.bin ~ (Cond*Gen) + (Gen * Shape) + (1 |Chain)  + (1|Item), 
            data=finalLangs, family=binomial, control = mcontrol)
mb6 = glmer(RatedSpikiness.bin ~ (Cond*Gen) + (Gen * Shape) + (Shape:Cond) + (1 |Chain)  + (1|Item), 
            data=finalLangs, family=binomial, control = mcontrol)
mb7 = glmer(RatedSpikiness.bin ~ Cond * Gen * Shape + (1 |Chain)  + (1|Item), 
            data=finalLangs, family=binomial, control = mcontrol)
```

### Results

Look inside main model

```{r}
summary(mb7)
```

Test model comparison:

```{r}
anova(mb0,mb1,mb2,mb3,mb4,mb5,mb6,mb7)
```

`r getMEText(anova(mb2,mb3), "main effect of shape",summary(mb7)$coef['ShapeSpiky',])`

`r getMEText(anova(mb5,mb6), "interaction between shape and condition", summary(mb7)$coef['CondCommunication:ShapeSpiky',])`

`r getMEText(anova(mb3,mb4), "interaction between shape and generation", summary(mb7)$coef['Gen:ShapeSpiky',])`

`r getMEText(anova(mb6,mb7), "three-way interaction between shape, condition and generation", summary(mb7)$coef['CondCommunication:Gen:ShapeSpiky',])`


Plot random effects of final model

```{r}
dotplot(ranef(mb7,  condVar=T))
```

Plot fixed effects with standard error from final model.

```{r message=F, warning=F}
sjp.lmer(mb7, type='fe', geom.colors=c(1,1))
```

\newpage

## Binary tree analysis

We use a binary decision tree to predict spikiness ratings by condition, generation, item shape, item colour and item border type.

The results agree with those above, namely that the main effects are for shape, but spiky meanings are rated as more spiky in the communication condition

```{r}
finalLangs2 = finalLangs
finalLangs2$Shape = factor(finalLangs2$Shape)
finalLangs2$Colour = factor(finalLangs2$Colour)
finalLangs2$Border = factor(finalLangs2$Border)
finalLangs2$Cond = factor(finalLangs2$Cond, labels = c("Reproduction",'Comm.'))

cx = ctree(RatedSpikiness~Cond+Gen+Shape+Colour+Border, data=finalLangs2)
plot(cx)

```

\newpage

# Iconicity of innovations

## Load data

Note that the column *Human* in the data indicates whether the signal was sent by a human.  This is always the case in the communication condition, but only true for half of the trials in the reproduction condition.  In the reproduction condition, when *Human* is `FALSE`, the human participant is guessing meaning from the signal sent by the program. 

```{r}
datax = read.csv("../results/IncreaseInIconicity.csv", stringsAsFactors = F)
alldatx = read.csv("../results/AllTrialData.csv", stringsAsFactors = F)
```

Number of innovations in each conditon, by whether the innovation was an unseen word (versus a change in mapping):

```{r}
tx = table(datax[datax$Human,]$innovation.mutation,
           datax[datax$Human,]$condition)
tx
tx/rep(colSums(tx),each=2)
```

The number of innovations in each condition by generation.  The number of innovations declines in the learning condition:

```{r}

par(mfrow=c(1,2))
barplot((table(
  datax[datax$condition=="Comm",]$innovation.mutation,
  datax[datax$condition=="Comm",]$gen)), beside = F,
  ylab="Number of innovations"
  ,ylim=c(0,150),
  col=1:2,
  main="Communicaiton",
  xlab="Generation")

lx = datax$condition=="Learn" & datax$Human

barplot((table(
  datax[lx,]$innovation.mutation,
  datax[lx,]$gen)), beside = F,
  ylab="Number of innovations"
  ,ylim=c(0,150),
  col=1:2,
  main="Reproduction",
  xlab="Generation")

legend(1,140,legend=c("Form","Mapping"),
  col=2:1, pch=15)


```



## Distribution of iconicity in innovations

Below is the distribution of how the innovation increases the iconicity of the mapping compared to the word that it replaced.  The increases are small compared to the full Likert scale (1-7), and centered around zero.

```{r iconicityProductionDistribution, cache=T, fig.height=5, fig.width=5}

# innovations produced by participants in the communication condition
comm.innovation.iconicity.dist = 
  datax[datax$condition=='Comm' & datax$Human,]$increaseIconicity
# innovations produced by the human in the reproduction condition
learn.innovation.iconicity.dist = 
  datax[datax$condition=='Learn' & datax$Human,]$increaseIconicity

cols = c('#1b9e77','#d95f02')

hist(comm.innovation.iconicity.dist, col=cols[1],
     breaks=14, 
     border = cols[1], 
     main='',
     xlab="Change in iconicity")
hist(learn.innovation.iconicity.dist, 
     add=T, 
     col=cols[2],
     breaks=14, 
     border = cols[2])
abline(v=0)
legend(-0.45,150, legend = c("Communication","Reproduction"), col=cols, pch=15, cex=0.8)

```


Are the distributions biased?  The curve for the learning condition looks like it has a bump on the right.  Test the symmetry with a Wilcox signed rank test and the MGG test (Miao, Gel & Gastwirth, 2006, see `symmetry.test` function in `lawstat` package).

```{r symmetryTest, cache=T}

wilcox.test(comm.innovation.iconicity.dist)
wilcox.test(learn.innovation.iconicity.dist)

symmetry.test(comm.innovation.iconicity.dist)
symmetry.test(learn.innovation.iconicity.dist)

```

The tests show that the distributions are not different from zero and are not asymmetric.  In other words, innovations are randomly distributed.  A mixed effects model also shows that the intercept is not significantly different from zero:

```{r}

m0 = lmer(increaseIconicity ~ condition*inFinalLang + (1|chain) + (1|gen) + (1|meaning),
          data=datax[datax$Human,])
sjp.lmer(m1,'fe',show.intercept=T)
```

Interestingly, the results look similar for the change in systematicity.

```{r systematicityProductionDistribution, cache=T}

# innovations produced by participants in the communication condition
comm.innovation.sys.dist = 
  datax[datax$condition=='Comm' & datax$Human,]$systematicity.increase
# innovations produced by the human in the reproduction condition
learn.innovation.sys.dist = 
  datax[datax$condition=='Learn' & datax$Human,]$systematicity.increase

cols = c('#1b9e77','#d95f02')

hist(comm.innovation.sys.dist, col=cols[1],
     breaks=14, 
     border = NA, 
     main='',
     xlab="Change in systematicity")
hist(learn.innovation.sys.dist, 
     add=T, 
     col=cols[2],
     breaks=14, 
     border = NA)
abline(v=0)
legend(-0.3,250, legend = c("Communication","Reproduction"), col=cols, pch=15)
```

```{r cache=T}
wilcox.test(comm.innovation.sys.dist)
wilcox.test(learn.innovation.sys.dist)

symmetry.test(comm.innovation.sys.dist)
symmetry.test(learn.innovation.sys.dist)
```


\newpage

### Innovations in form versus innovations in mapping

There appear to be no differences with regards to iconiciy according to the category of innovation:

```{r}

plotmeans(increaseIconicity ~ paste(condition,innovation.mutation), 
          data=datax,
          legends = c("Mapping","Form","Mapping",'Form'),
          xlab='',
          ylab='Increase in iconicity',
          connect = list(1:2,3:4)
          )
axis(1,at=c(1.5,3.5),c("Communication","Reproduction"),line=1, tick=F)
abline(h=0)

```

However, there seems like a weak bias for form innovations in the learning condition to lead to an increase in systematicity.  Future work could explore these implications.

```{r}
plotmeans(systematicity.increase ~ paste(condition,innovation.mutation), 
          data=datax,
          legends = c("Mapping","Form","Mapping",'Form'),
          xlab='',
          ylab='Increase in systematicity',
          connect = list(1:2,3:4)
          )
axis(1,at=c(1.5,3.5),c("Communication","Reproduction"),line=1, tick=F)
abline(h=0)

```

\newpage

## Increase in iconicity by survival

In the communication condition, innovations that survive tend to increase both iconicity and systematicity.  However, in the learning condition, the innovations only contribute to systematicity but not to iconicity.

```{r}
par(mfrow=c(1,2))
plotmeans(increaseIconicity ~ paste(condition,inFinalLang), data=datax[datax$Human,], connect = list(1:2,3:4),xlab='', ylab="Increase", legends = c("Rejected","Survived","Rejected","Survived"))
title(main="Iconicity")
axis(1,at=c(1.5,3.5),c("Communication","Reproduction"),line=1, tick=F)
abline(h=0)
plotmeans(systematicity.increase ~ paste(condition, inFinalLang), data = datax[datax$Human,], connect = list(1:2,3:4), xlab='', ylab="Increase",, legends = c("Rejected","Survived","Rejected","Survived"))
title(main="Systematicity")
axis(1,at=c(1.5,3.5),c("Communication","Reproduction"),line=1, tick=F)
abline(h=0)
```

Build a mixed effects model predicting the increase in iconicity with random effects for chain, generation and item:

```{r}
m0= lmer(increaseIconicity ~ 1 +(1|chain) + (1|meaning) + (1|gen),
         data=datax[datax$Human,])
m1= lmer(increaseIconicity ~ condition +(1|chain) + (1|meaning) + (1|gen),
         data=datax[datax$Human,])
m2= lmer(increaseIconicity ~ condition+inFinalLang +(1|chain) + (1|meaning) + (1|gen),
         data=datax[datax$Human,])
m3= lmer(increaseIconicity ~ condition*inFinalLang +(1|chain) + (1|meaning) + (1|gen),
         data=datax[datax$Human,])
```

Model comparison test:

```{r}
anova(m0,m1,m2,m3)
```

`r getMEText(anova(m2,m3),"interaction between condition and survival to transmission",summary(m3)$coef['conditionLearn:inFinalLangTRUE',])`

Plot the fixed effects from the final model:

```{r}
sjp.lmer(m3,'fe', geom.colors = c(1,1), p.kr=F)
```



### Increase in systematicity by survival

Build a mixed effects model predicting the increase in systematicity with random effects for chain, generation and item:

```{r}
m0= lmer(systematicity.increase ~ 1 +(1|chain) + (1|meaning) + (1|gen),
         data=datax[datax$Human,])
m1= lmer(systematicity.increase ~ condition +(1|chain) + (1|meaning) + (1|gen),
         data=datax[datax$Human,])
m2= lmer(systematicity.increase ~ condition+inFinalLang +(1|chain) + (1|meaning) + (1|gen),
         data=datax[datax$Human,])
m3= lmer(systematicity.increase ~ condition*inFinalLang +(1|chain) + (1|meaning) + (1|gen),
         data=datax[datax$Human,])
```

Model comparison test:

```{r}
anova(m0,m1,m2,m3)
```

`r getMEText(anova(m2,m3),"main effect of condition",summary(m3)$coef['conditionLearn:inFinalLangTRUE',])`

`r getMEText(anova(m2,m3),"interaction between condition and survival to transmission",summary(m3)$coef['conditionLearn:inFinalLangTRUE',])`

Plot the fixed effects from the final model:

```{r}
sjp.lmer(m3,'fe', geom.colors = c(1,1), p.kr = F)
```


\newpage

# Accuracy

The mean proportion of correct guesses in the communication condition was `r round((sum(alldatx[alldatx$condition=="Comm",]$correctGuess) / sum(alldatx$condition=="Comm"))*100,2)
`%.  The mean proportion of correct guesses by the human participant in the learning condition was `r round((sum(alldatx[alldatx$condition=="Learn" & !alldatx$Human,]$correctGuess) / sum(alldatx$condition=="Learn" & !alldatx$Human))*100,2)`%.

Plot the correct guesses by generation (means and 95% confidence intervals):

```{r warning=F}
plotmeans(correctGuess~gen,alldatx[alldatx$condition=='Learn' & !alldatx$Human,], n.label = F)
plotmeans(correctGuess~gen,alldatx[alldatx$condition=='Comm',],add=T,col=2,barcol=2, n.label = F)
```


## Mixed effects model

Binomial mixed effects model, with random effects for chain, target item.  Test whether there are differences between conditions.

```{r}

ctrl = glmerControl(optCtrl = list(maxfun=50000))
# we want to exclude trials where the computer is guessing meanings 
# from the participant's signals in the reproduction condition
m0 = glmer(correctGuess ~ 1 + (1|chain) + (1|target.meaning) , 
    data=alldatx[alldatx$condition=='Comm' | (!alldatx$Human),],
    family = binomial, control= ctrl)

m1 = glmer(correctGuess ~ 1 + (1|chain) + (1|target.meaning) + (1|gen), 
    data=alldatx[alldatx$condition=='Comm' | (!alldatx$Human),], 
    family = binomial, control= ctrl)
m2 = glmer(correctGuess ~ condition + (1|chain) + (1|target.meaning)+ (1|gen), 
    data=alldatx[alldatx$condition=='Comm' | (!alldatx$Human),], 
    family = binomial, control= ctrl)
anova(m0,m1,m2)
```

`r getMEText(anova(m1,m2),ef = "main effect of condition", summary(m2)$coef[2,])
`

`r getMEText(anova(m0,m1),ef = "difference between generations")`  There is a weak trend for the proportion of correct guesses to increase by generation, as shown by the estimates for the random effects for generation:

```{r message=F, warning=F}
x = sjp.lmer(m2, sort.est='sort.all', prnt.plot=F,
             axis.labels=c("Chain",'Item','Generation'),
             geom.colors=c(1,1))
x$plot.list[[3]] + 
  xlab("Generation") + 
  ylab("Random effect (fit of increase in correct guesses)")
```

\newpage

## Iconicity and accuracy for innovations

Innovations are either more or less iconic than the words they replace.  There is no difference in how accurate the guesses are in terms of choosing the right item (see below, left), but the innovation tends to be more iconic when the shape of a meaning is guessed correctly (spiky or round).  That is, the iconicity is helping participants guess the shape of a target meaning correctly.

Note that this analysis only makes sense for the communication condition. 

```{r}
par(mfrow=c(1,2))
ylimx = c(-0.045,0.045)
plotmeans(increaseIconicity ~ paste(condition, correctGuess), 
          data = datax[datax$Human & datax$condition=="Comm",], 
          ylim=ylimx, legends = c("Incorrect","Correct"),
          xlab='',
          ylab="Increase in iconicity")
title("Guessing Item")
abline(h=0)
plotmeans(increaseIconicity ~ paste(condition, correctSpikiness), 
          data = datax[datax$Human& datax$condition=="Comm",], 
          ylim=ylimx,legends = c("Incorrect","Correct"),
          xlab='',
          ylab="Increase in iconicity")
title("Guessing Shape")
abline(h=0)
```

\newpage

### Mixed effects model for accuracy and iconicity

A mixed effects model predicting the increase in iconicity by whether the reciever selected the correct target item, and by whether the reciever selected an item which matched the target in the shape dimension, with random effects for chain, generation and item.  Note that it would make more intuitive sense to predict accuracy by increase in iconicity, but this way we can compare the effects of item accuracy versus shape accuracy.

```{r}
m0 = lmer(increaseIconicity ~ 1 + (1|chain) + (1|gen) + (1|meaning), 
          data=datax[datax$condition=="Comm",])

m1 = lmer(increaseIconicity ~ correctGuess + (1|chain) + (1|gen)+ (1|meaning), 
          data=datax[datax$condition=="Comm",])

m2 = lmer(increaseIconicity ~ correctGuess + correctSpikiness + (1|chain) + (1|gen)+ (1|meaning), 
          data=datax[datax$condition=="Comm",])

anova(m0,m1,m2)

summary(m2)
```

`r getMEText(anova(m0,m1), "main effect of guessing the item correctly",summary(m2)$coef['correctGuessTRUE',])`

`r getMEText(anova(m1,m2), "main effect of guessing the shape correctly",summary(m2)$coef['correctSpikinessTRUE',])`



Plot the fixed effects:

```{r message=F, warning=F}
sjp.glmer(m2, type='fe', geom.colors=c(1,1) )
```

Note that the model is probably overfitted, since the random effects are singulative. But the effect is clear from the plot of the raw data.

## Iconicity and accuracy for whole data

Make a variable that indicates the iconicity of a word according to its shape.  i.e. high if it aligns with shape, low if it does not.  In other words, reverse the scale for round meanings.

```{r}
alldatx$estimatedIconicity = alldatx$estimatedSpikinessRating

alldatx$estimatedIconicity[alldatx$target.meaning>5] = 
  7 - alldatx$estimatedIconicity[alldatx$target.meaning>5]

```

Plot the raw data

```{r}
par(mfrow=c(1,2))
ylimx= c(3.4,3.65)
plotmeans(estimatedIconicity~correctSpikiness, 
          data=alldatx[alldatx$condition=='Comm',],
          ylim = ylimx,
          xlab="Guessing shape",
          ylab="Iconicity",
          legends = c("Incorrect","Correct"),
          main="Communication")

plotmeans(estimatedIconicity~correctSpikiness, 
          data=alldatx[alldatx$condition=='Learn' &
                         (!alldatx$Human),],
          ylim = ylimx,
          xlab="Guessing shape",
          ylab="Iconicity",
          legends = c("Incorrect","Correct"),
          main="Reproduction")


```




\newpage

# Iconicity and systematicity

We would like to test whether there is a link between the increase in iconicity and the increase in systematicity for innovations.

Overall correlation between systematicity increase and iconicity increase:

```{r}
cor.test(datax[datax$Human,]$increaseIconicity,datax[datax$Human,]$systematicity.increase)
```

For the communication condition:

```{r}
cor.test(
  datax[datax$condition=="Comm",]$increaseIconicity,
  datax[datax$condition=="Comm",]$systematicity.increase)
```

For the reproduction condition:

```{r}
cor.test(
  datax[datax$condition=="Learn" & datax$Human,]$increaseIconicity,
  datax[datax$condition=="Learn" & datax$Human,]$systematicity.increase)
```

Simple linear model:

```{r}
summary(lm(increaseIconicity~systematicity.increase*condition, data=datax[datax$Human,]))
```

Significant main effect for systematicity increase and interaction.

## Mixed effects model

Build a series of mixed effects models predicting the increase in iconicity with random effects for chain and item.

```{r}

# select data - either trails from the communication condition
#  or from the reproduction condition when the human was the speaker
datax.sel = datax[datax$Human,]

# Null model, predicting increase in iconicity by condition and generation
m0 = lmer(increaseIconicity~condition*gen + (1|chain) + (1|meaning),
          data = datax.sel)

# Add main effect of systematicity increase
m1 = lmer(increaseIconicity~systematicity.increase+(condition*gen) + (1|chain) + (1|meaning), 
          data = datax.sel)

# Add interaction between Sys. and condition
m2 = lmer(increaseIconicity~systematicity.increase + (systematicity.increase:condition) +
            (condition*gen) + (1|chain) + (1|meaning), 
          data = datax.sel)

# Add 3-way interaction
m3 = lmer(increaseIconicity~systematicity.increase*condition*gen +
            (1|chain) + (1|meaning), 
          data = datax.sel)
```

## Results

Test the model fit.  Note that the model converges on signualtive random effect estimates.

```{r}
anova(m0,m1,m2,m3)
```

`r getMEText(anova(m1,m2),"interaction between systematicity increase and condition")`

Plot the fixed effects of model 2.  The relationship between systematicity and iconicity is more negative in the reproduction condition:

```{r}
sjp.lmer(m2,type='fe', p.kr = F)
```

Plot differences in raw data:

```{r}
ylimx= c(-0.015,0.018)
par(mfrow=c(1,2))
plotmeans(systematicity.increase ~ increaseIconicity>0,
          data=datax[datax$condition=="Comm",], 
          legends = c("Decrease","Increase"), 
          xlab="Change in iconicity", 
          ylab="Change in systematicity",
          main = "Communication",
          ylim = ylimx)
abline(h=0)
plotmeans(systematicity.increase ~ increaseIconicity>0,
          data=datax[datax$Human & datax$condition=="Learn",], 
          legends = c("Decrease","Increase"), 
          xlab="Change in iconicity", 
          ylab="Change in systematicity",
          main = "Reproduction",
          ylim = ylimx)

abline(h=0)
```



